---
layout: post-math-cn
title:  "线代导论 06：特征值和特征向量"
categories: 线性代数
tags: 线性代数导论 特征值 特征向量 Markov矩阵 迹 对角化 Fibonacci 微分方程 对称矩阵 正定矩阵
author: buzzyrain
mathjax: true
---

* content
{:toc}

线性代数的前半部分都是关于 $ A \boldsymbol{x} = \boldsymbol{b} $，描述了**平衡稳定**的系统；往后我们将关注**动态**系统，当**时间**参与方程，消元法就失效了，
- 连续（continuous）时间的微分方程（differential equation）$ d\boldsymbol{u}/dt=A \boldsymbol{u} $
- 离散（discrete）时间的差分方程（difference equation）$ \boldsymbol{u}_{k+1} = A \boldsymbol{u}_k $

为了简化矩阵 $ A $ 带来的复杂度，假设解 $ \boldsymbol{u}(t) $ 与一固定向量 $ \boldsymbol{x} $ **同向**，那么我们仅需要找到**一个（随时间变化的）数**乘上 $ \boldsymbol{x} $。

**固定向量 $ \boldsymbol{x} $ 称为"特征向量（eigenvalue）"，矩阵 $ A $ 的作用不改变它的方向**。

尽管几乎所有向量左乘 $ A $ 时都会改变方向，但确实有特殊的（非常突出的性质，所以叫“特征”） $ \boldsymbol{x} $ 与 $ A \boldsymbol{x} $ 同向，

$$ A \boldsymbol{x} = \lambda \boldsymbol{x} $$

$ \lambda $ 是**特征值（eigenvalue）**，告诉我们特征向量 $ \boldsymbol{x} $ 是被拉伸、压缩、反向还是根本没变，比如对应的 $ \lambda=2,\ \cfrac12,\ -1,\ 1 $。$ \lambda $ 不能为零，$ A \boldsymbol{x} = \boldsymbol{0} $ 说明 $ \boldsymbol{x} $ 在 $ \mathbb{N}(A) $ 中，没用。

---

$$ \mathrm{det}\ (A-\lambda I)=\boldsymbol{0} $$

我们用这个式子来找特征向量 $ \boldsymbol{x}'s $、特征值 $ \lambda's $。




<br>

以**Markov（马尔可夫）**矩阵为例来感受特征值和特征向量：

![markov-matrix-eigen](https://wx3.sinaimg.cn/large/9f1c5669ly1fvkduopr0dj20ta0bhdjj.jpg "Markov Matrix")

- $ A \boldsymbol{x}_1 = \boldsymbol{x}_1 $，$ A $ 对向量 $ \boldsymbol{x}_1 $ 完全没有影响，所以不管乘上多少个 $ A $，都有 $ A^n \boldsymbol{x}_1 = \boldsymbol{x}_1 $
- $ A \boldsymbol{x}_2 = \frac12 \boldsymbol{x}_2 $，每乘一次 $ A $，$ \boldsymbol{x}_2 $ 都会被压缩一半，但方向没变，只改变特征值（长度）就行，即 $ A^n \boldsymbol{x}_2 = (\frac12)^n \boldsymbol{x}_2 $

对于其它普通向量，方向都是会变的，但是我们可以用这两个特征向量来表示！

比如第一列的 $$ \begin{bmatrix}  .8 \\ .2 \\ \end{bmatrix} = \boldsymbol{x}_1 + 0.2 \boldsymbol{x}_2 = \begin{bmatrix}  .6 \\ .4 \\ \end{bmatrix} + \begin{bmatrix}  .2 \\ -.2 \\ \end{bmatrix} $$

$$ A \begin{bmatrix}  .8 \\ .2 \\ \end{bmatrix} = 1 \cdot \boldsymbol{x}_1 + 0.5 \cdot (0.2 \boldsymbol{x}_2) = \begin{bmatrix}  .6 \\ .4 \\ \end{bmatrix} +  0.5 \cdot \begin{bmatrix}  .2 \\ -.2 \\ \end{bmatrix} = \begin{bmatrix}  .7 \\ .3 \\ \end{bmatrix} $$

看起来比直接算 $$ A \begin{bmatrix}  .8 \\ .2 \\ \end{bmatrix} $$ 麻烦，但要是算 $$ A^{99} \begin{bmatrix}  .8 \\ .2 \\ \end{bmatrix} $$ 呢 🌚

$$ A^{99} \begin{bmatrix}  .8 \\ .2 \\ \end{bmatrix} = 1^{99} \cdot \boldsymbol{x}_1 + (0.5)^{99} \cdot (0.2 \boldsymbol{x}_2) = \begin{bmatrix}  .6 \\ .4 \\ \end{bmatrix} + \begin{bmatrix} \mathrm{infinitesimal} \\ \mathrm{vector} \\ \end{bmatrix} = \begin{bmatrix}  .6 \\ .4 \\ \end{bmatrix} $$

- $$ \boldsymbol{x}_1 =  \begin{bmatrix}  .6 \\ .4 \end{bmatrix} $$ 是一个稳定（steady）的特征向量，因为怎么乘 $ A $ 都不会变（$ \lambda_1=1 $）
- $$ \boldsymbol{x}_2 =  \begin{bmatrix}  .1 \\ -.1 \end{bmatrix} $$ 是一个衰减（decay）的特征向量，因为 $ \lambda_2=.5 $，乘的 $ A $ 越多，$ \lim_{n \to \infty} (0.5)^n=0 $

这种特殊的矩阵 $ A $ 叫 Markov 矩阵（每列加起来是$ 1 $），它最大的特征值就是 $ \lambda=1 $，对应的 $ \boldsymbol{x} $ 就是 $ A $ 的列向量最后趋向的稳定状态。

![markov-steady](https://wx3.sinaimg.cn/large/9f1c5669ly1fvkeqlu1cij20nc0253z9.jpg "Markov Steady")


## 计算特征值

像投影矩阵 $ P \boldsymbol{x} = \boldsymbol{x},\ P \boldsymbol{x} = \boldsymbol{0} $，我们一眼可以看出 $ \lambda_1 = 1,\ \lambda_1 = 0 $，但是其它不那么特殊的矩阵，就要从行列式和线性代数的角度入手，解方程 $ A \boldsymbol{x} = \lambda \boldsymbol{x} $。

方程可以改写为 $ (A - \lambda I) \boldsymbol{x} = \boldsymbol{0} $，矩阵 $ (A - \lambda I) $ 乘上特征向量 $ \boldsymbol{x} $ 是零向量，表示特征向量在 $ \mathbb{N}(A-\lambda I) $ 中。如果知道了 $ \lambda $，就可以解出 $ \boldsymbol{x} $。

- 特征值

    如果 $ (A - \lambda I) \boldsymbol{x} = \boldsymbol{0} $ **要有非零解**，那么矩阵 $ (A - \lambda I) $ **就得是**奇异的。所以 $ \mathrm{det}\ (A - \lambda I) = 0 $。

    **特征多项式（characteristic polynomial）** $ (A - \lambda I) $ 仅是 $ \lambda $ 的多项式方程，$ \boldsymbol{x} $ 不会出现。当 $ A $ 是 $ n \times n $ 的矩阵，它的特征多项式就是 $ n $ 阶，方程会有 $ n $ 个解（**特征值有可能重复**）。

- 特征向量

    每个 $ \lambda $ 对应一个 $ \boldsymbol{x} $，只需要解出 $ (A - \lambda I) \boldsymbol{x} = \boldsymbol{0} $ 或者 $  A \boldsymbol{x} = \lambda \boldsymbol{x} $，所以特征向量 $ \boldsymbol{x} $ 也有了。


举例，求奇异矩阵 $$ A =  \begin{bmatrix} 1&2  \\ 2&4 \\ \end{bmatrix} $$ 的 $ \lambda's,\ \boldsymbol{x}'s $。

首先，奇异矩阵一定有非零解使得 $ A \boldsymbol{x} = \boldsymbol{0} $，所以有一个 $ \lambda_1 = 0 $，此时的 $$ \boldsymbol{x}_1 =  \begin{bmatrix}  2 \\ -1 \\ \end{bmatrix} $$。

+ 但是为了找到所有的**特征值**，还是老老实实的解 $ \mathrm{det}\ (A-\lambda I)=\boldsymbol{0} $ 💩

    $$
    \begin{align}
    \mathrm{det}\ \begin{vmatrix}  1-\lambda & 2 \\ 2 & 4-\lambda \\ \end{vmatrix}
    &= (1-\lambda)(4-\lambda)-(2)(2)
    &=\lambda^2 - 5 \lambda
    \end{align}
    $$

    令行列式为零（需要奇异），解出 $ \lambda_1 = 0,\ \lambda_2 = 5 $。正如我们所预料的，有一个特征值是 $ 0 $

+ 下面用特征值解**特征向量**

    $ \lambda_1 = 0 $ 代入得 $$ (A - 0I)\boldsymbol{x}= \begin{bmatrix} 1&2  \\ 2&4 \\ \end{bmatrix} \begin{bmatrix} y \\ z \\ \end{bmatrix}=\begin{bmatrix} 0 \\ 0 \\ \end{bmatrix} $$，解出 $$  \begin{bmatrix} y \\ z \\ \end{bmatrix} = \begin{bmatrix} 2 \\ -1 \\ \end{bmatrix} $$

    $ \lambda_2 = 5 $ 代入得 $$ (A - 5I)\boldsymbol{x}= \begin{bmatrix} -4&2  \\ 2&-1 \\ \end{bmatrix} \begin{bmatrix} y \\ z \\ \end{bmatrix}=\begin{bmatrix} 0 \\ 0 \\ \end{bmatrix} $$，解出 $$  \begin{bmatrix} y \\ z \\ \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ \end{bmatrix} $$

$ \lambda=0 $ 不是什么大事，如果 $ A $ 是奇异的，特征向量就满足 $ A \boldsymbol{x} = \boldsymbol{0} $，填充了 $ \mathbb{N}(A) $，如果 $ A $ 是可逆的，则 $ \lambda \neq 0 $，所以要将 $ A $ **移动（shift）** $ \lambda $ 个单位 $ I $ 使它**变得奇异**。

这个例子里的 $ \lambda_2=5 $ 就是移动了 $ 5 $ 个 $ I $，使得 $ A-5I $ 奇异，所以 $ 5 $ 确实是另一个特征值。


### 行列式和迹

特征值往往会在消元的过程中改变，上三角矩阵 $ U $ 的特征值就沿着对角线（主元），但它们却不是 $ A $ 的特征值。

看这个例子，$$ U =  \begin{bmatrix} 1&3  \\ 0&0 \\ \end{bmatrix} $$  的特征值是 $ \lambda_1=0,\ lambda_2=1 $；而将第一行翻倍加到第二行之后（行线性操作） $$ A= \begin{bmatrix} 1&3 \\ 2&6 \\ \end{bmatrix} $$ 有 $ \lambda_1=0,\ lambda_2=7 $。

但是特征值还有一个不错的性质，$ \lambda_1 \cdot \lambda_2 $ 和 $ \lambda_1 + \lambda_2 $ 可以很快从矩阵中看出来，分别等于矩阵的行列式和**迹（trace）**：
- $ \mathrm{det}\ = \lambda_1 \lambda_2 \cdots \lambda_n $
- $ \mathrm{trace}\ = \lambda_1 + \lambda_2 + \cdots + \lambda_n $

所谓迹就是对角线元素之和，$ \mathrm{trace}\ = \lambda_1 + \lambda_2 + \cdots + \lambda_n = a_{11} + a_{22} + \cdots + a_{nn} $

这俩性质可以帮助我们验算。

实际上 $ 2 \times 2 $ 的矩阵，$ \mathrm{det}\ 和 \mathrm{trace} $ 就足够解出 $ \lambda_1,\ \lambda_2 $（两个方程两个解）。


### 虚特征值

$ \lambda $ 可能不是实数，是**虚的（imaginary）**。

90°的旋转矩阵 $$ Q= \begin{bmatrix} 0&-1 \\ 1&0 \\ \end{bmatrix} $$ 就没有实特征向量，它的两个特征值是 $ \lambda_1=i,\lambda_2=-i $。根据刚才提到的行列式和迹的检验，$ \lambda_1 + \lambda_2 = i - i = 0 $，$ \lambda_1 \cdot \lambda_2 = (i)(-i)=1=\mathrm{det}\ Q $。

**旋转**矩阵，没有一个实数向量可以待在同一方向，只有 $ \boldsymbol{x}=\boldsymbol{0} $，但 $ \boldsymbol{0} $ 没用。所以我们寻求虚数的帮助。


1. 直观解释虚特征值

    因为 $ Q $ 旋转了90°，那么 $ Q^2 = -I $ 将旋转180°，$ -I $ 的特征值是 $ \lambda_1=-1,\ lambda_2=-1 $，$ Q $ 平方的同时 $ \lambda $ 也取了平方，所以我们必须有 $ \lambda^2=-1 $，即 $ Q $ 的特征值是 $ \pm i $

2. 解 $ \mathrm{det}\ (Q-\lambda I)=\boldsymbol{0} $

    $ \lambda^2+1=0 \to \lambda=\pm i $

找到虚特征值之后求特征向量，

$$ \begin{bmatrix} 0 & -1 \\ 1 & 0 \\ \end{bmatrix} \begin{bmatrix} 1 \\ i \\ \end{bmatrix}=-i \begin{bmatrix} 1 \\ i \\ \end{bmatrix} $$ 和 $$ \begin{bmatrix} 0 & -1 \\ 1 & 0 \\ \end{bmatrix} \begin{bmatrix} i \\ 1 \\ \end{bmatrix}=i \begin{bmatrix} i \\ 1 \\ \end{bmatrix} $$

这两个特征向量 $ \boldsymbol{x}_1 = (1,\ i),\ \boldsymbol{x}_2 = (i,\ 1) $ 在旋转后方向不变。

其实特征值 $ \pm i $ 揭示了 $ Q $ 的两个特殊性质
- $ Q $ 是一个正交矩阵，每个特征值的绝对值都是 $ \lvert \lambda \rvert =1 $。
- $ Q $ 是偏对称矩阵（skew-symmetric matrix），即 $ Q^T = -Q $，所以每个 $ \lambda $ 都是纯虚数。

**一些矩阵特殊的性质（对称、正交等等）会有特殊的特征值和特征向量。**


### 练习

**Q1**. 如果 $ A $ 的特征值和特征向量分别是 $ \Lambda,\ \boldsymbol{X} $（矩阵形式），那么 $ A^2,\ A^{-1},\ A+4I $ 的特征值/向量分别是？

 $ A^2,\ A^{-1},\ A+4I $ 的特征向量与 $ A $ 相同，都是 $ \boldsymbol{X} $，只是特征值变为 $ \Lambda^2,\ \Lambda^{-1} $ 和 $ \Lambda + 4I $

 **Q2**. 找到这个 $ 3 \times 3 $ 矩阵 $$ S=\begin{bmatrix} 1&-1&0\\ -1&2&-1\\ 0&-1&1 \\ \end{bmatrix} $$ 的特征值和特征向量。

迹等于 $ 1+2+1=4 $，待会儿可以检查特征值。

发现每行加起来都是 $ 0 $（奇异矩阵），所以 $ \boldsymbol{x}=(1,\ 1,\ 1) $ 满足 $ S \boldsymbol{x} = \boldsymbol{0} $，此时的 $ \lambda_1=0 $

$ \lambda_2,\lambda_3 $ 还是要算行列式方程：

$$ \mathrm{det}\ (S-\lambda I)= \begin{vmatrix} 1-\lambda &-1&0\\ -1&2-\lambda &-1\\ 0&-1&1-\lambda \\ \end{vmatrix} = (1-\lambda)(-\lambda)(3-\lambda)$$，所以有 $ \lambda=0,\ 1,\ 3 $。

对应的特征向量是 $$ \lambda_1=0,\ \boldsymbol{x}_1 =  \begin{bmatrix} 1  \\ 1 \\ 1 \\ \end{bmatrix} $$，$$ \lambda_2=1,\ \boldsymbol{x}_2 =  \begin{bmatrix} 1  \\ 0 \\ -1 \\ \end{bmatrix} $$，$$ \lambda_3=3,\ \boldsymbol{x}_1 =  \begin{bmatrix} 1  \\ -2 \\ 1 \\ \end{bmatrix} $$。

再大的矩阵肯定不用手算了，`eig(A)`就能解决（**MATLAB**），也不用接触这坨行列式。

**Q3**. 一个 $ 3 \times 3 $ 的矩阵 $ B $ 有特征值 $ 0,\ 1,\ 2 $，这些信息可以告诉我们
1. $ B $ 的秩：$ 2 $，因为有**一个** $ \lambda=0 $，$ B $ 是奇异矩阵。
2. $ B^TB $ 的行列式：$ \mathrm{det}\ (B^TB) = \lvert B^T \rvert \lvert B \rvert = 0 $。
3. $ B^TB $ 的特征值：信息不够，没法知道。
4. $ (B^2 + I)^{-1} $ 的特征值：$ \lambda_1=1/(0^2+1)=1$，$ \lambda_2=1/(1^2+1)=\frac12$，$  \lambda_3=1/(3^2+1)=\frac15 $。


**Q4**. 假设 $ A $ 有特征值 $ 0,\ 3,\ 5 $ 和对应的线性无关的 $ \boldsymbol{u},\ \boldsymbol{v},\ \boldsymbol{w} $
1. $ \mathbb{N}(A) $ 的基向量是【$ \boldsymbol{u} $】；$ \mathbb{C}(A) $ 的基向量是【$ \boldsymbol{v},\ \boldsymbol{w} $】
2. $ A \boldsymbol{x} = \boldsymbol{v} + \boldsymbol{w} $ 的特解可以表示成？

    我们知道 $ A \boldsymbol{v} = 3 \boldsymbol{v},\ A \boldsymbol{w} = 5 \boldsymbol{w} $，所以 $ \boldsymbol{v} + \boldsymbol{w} = \cfrac{1}{3}A \boldsymbol{v} + \cfrac{1}{5}A \boldsymbol{w} = A (\cfrac{1}{3} \boldsymbol{v} + \cfrac{1}{5}\boldsymbol{w}) $，即 $ \boldsymbol{x} = \cfrac{1}{3} \boldsymbol{v} + \cfrac{1}{5}\boldsymbol{w} $ 是一个特解。
3. $ A \boldsymbol{x} = \boldsymbol{u} $ 是没有解的，因为 $ \boldsymbol{u} $ 在 $ \mathbb{N}(A) $ 而不是 $ \mathbb{C}(A) $。


**Q5**. **Heisenberg's Uncertainty Principle 海森堡不确定性原理** $ AB-BA=I $ 可以在无限矩阵 $ A=A^T,\ B=-B^T $ 的条件下满足，这时 $ \boldsymbol{x}^T \boldsymbol{x}=\boldsymbol{x}^TAB \boldsymbol{x} - \boldsymbol{x}^TBA \boldsymbol{x} \leq 2 \lVert \boldsymbol{A \boldsymbol{x}} \rVert \lVert \boldsymbol{B}\boldsymbol{x} \rVert $。

用 Schwartz 不等式 $ \lvert \boldsymbol{u}^T \boldsymbol{v} \rvert \leq \lVert \boldsymbol{u} \rVert \lVert \boldsymbol{v} \rVert $ 解释最后一步。

Heisenberg's inequality 是说 $ \lVert \boldsymbol{A \boldsymbol{x}} \rVert / \lVert \boldsymbol{x} \rVert $ 与 $ \lVert \boldsymbol{B \boldsymbol{x}} \rVert / \lVert \boldsymbol{x} \rVert $ 的乘积至少等于 $ \frac12 $。也就是，位置和动量的误差不可能同时很小，没法同时精确得到位置和动量的信息。

![Heisenberg-inequality](https://wx4.sinaimg.cn/large/9f1c5669ly1fvkmqiyd8nj20do08pt9u.jpg "Heisenberg Inequality")


## 对角化矩阵


$ \boldsymbol{x} $ 如果是一个特征向量，它乘上 $ A $ 的效果就和乘一个数 $ \lambda $ 一样，即 $ A \boldsymbol{x} = \lambda \boldsymbol{x} $。接着把 $ \lambda's $ 都放在对角线上，我们就可以相对**独立**的处理每个特征向量，因为对角矩阵没有**行列纠缠（interconnected）**的非对角线元素，就像 $ Diag^{100} $ 很容易求。

所以如果能利用特征值/向量打扮一番 $ A $，让她变成一个对角矩阵方便计算，这就是我们的目标。

学名为**对角化（diagonalization）**，假设 $ A $ 有 $ n $ 个线性无关的特征向量 $ \boldsymbol{x}_1,\ \cdots,\ \boldsymbol{x}_n $，将它们作为列向量放进**特征向量矩阵（eigenvector matrix） $ X $**。那么 $ X^{-1}AX $ 就是**特征值矩阵（eigenvalue matrix） $ \Lambda $**（大写的 $ \lambda $，表示矩阵）：

$$ X^{-1}AX = \Lambda = \begin{bmatrix} \lambda_1 & &  \\ & \ddots & \\ & & \lambda_n \end{bmatrix} $$

这样的 $ A $ 就是**可对角化**的 $ A $。

---

为什么 $ X^{-1}AX = \Lambda $ 或者说 $ AX=X \Lambda $？

等式左侧 $ AX $：$$ AX = A \begin{bmatrix} \boldsymbol{x}_1 & \cdots & \boldsymbol{x}_n \end{bmatrix} =  \lambda \begin{bmatrix} \boldsymbol{x}_1 & \cdots & \boldsymbol{x}_n \end{bmatrix} $$

证等右侧 $ X \Lambda $：$$ \begin{bmatrix} \lambda_1 \boldsymbol{x}_1 & \cdots & \lambda_n \boldsymbol{x}_n \end{bmatrix} = \begin{bmatrix} \boldsymbol{x}_1 & \cdots & \boldsymbol{x}_n \end{bmatrix} \begin{bmatrix} \lambda_1 & &  \\ & \ddots & \\ & & \lambda_n \end{bmatrix} = X\Lambda $$

所以 $ AX=X \Lambda $，即证 $ X^{-1}AX = \Lambda $ 和 $ A=X\Lambda X^{-1} $

⚠️ 特征向量与特征值要**一一对应**，如 $ \lambda_1 $ 一定和 $ \boldsymbol{x}_1 $ 结伴。

当我们看似名正言顺的写下 $ X^{-1} $ 时，已经默认了 $ X $ 是可逆的，即特征向量 $ \boldsymbol{x}'s $ 互相独立。如果这个条件不满足，**没有线性无关的特征向量，就不可以对角化**。

对角化矩阵有何用呢？

👉 $ A^k = (X\Lambda X^{-1})(X\Lambda X^{-1}) \cdots (X\Lambda X^{-1})(X\Lambda X^{-1}) = X\Lambda^k X^{-1} $

有几个注意事项

1. 如果 $ \lambda_1,\ \cdots,\ \lambda_n $ 全不一样，明显表明特征向量 $ \boldsymbol{x}_1,\ \cdots,\ \boldsymbol{x}_n $ 互相独立，即特征向量矩阵 $ X $ 可逆。**任何没有重复特征值的向量都可以对角化**。
2. 特征向量可以乘上任何非零常数。$ A(c \boldsymbol{x}) = c A \boldsymbol{x}$ 仍然成立。所以 $ \boldsymbol{x}'s $ 是可以单位化的，除以自身的长度后 $ \lVert \boldsymbol{x} \rVert $=1 。
3. 只要特征向量与特征值配对好了，特征向量**之间的顺序**是无所谓的，我们也可以写成 $$ X'\Lambda'= \begin{bmatrix} \boldsymbol{x}_n & \cdots & \boldsymbol{x}_1 \end{bmatrix} \begin{bmatrix} \lambda_n & &  \\ & \ddots & \\ & & \lambda_1 \end{bmatrix} $$。
4. 特征值如果重复出现了，说明特征向量**不足够**对角化。

    $$ \mathrm{det}\ A=  \begin{vmatrix}  1 & -1 \\ 1 & -1 \\ \end{vmatrix} = (1-\lambda)(-1-\lambda)+1 = \lambda^2 - 1 + 1 = \lambda^2 = 0 $$

    $$ \mathrm{det}\ B=  \begin{vmatrix}  0 & 1 \\ 0 & 0 \\ \end{vmatrix} = \lambda^2 - 0 = \lambda^2 $$

    矩阵 $ A,\ B $ 的特征值都是 $ 0,\ 0 $，我们说 $ \lambda=0 $ 没什么可怕的，但是 $ \lambda_1=\lambda_2=0 $，**重复**的特征值是有问题的。

    $ A $ 所有的特征向量都是 $$ \begin{bmatrix}  1 \\ 1 \\ \end{bmatrix} $$ 的倍数，$$ \boldsymbol{x} =  c \begin{bmatrix} 1  \\ 1 \\ \end{bmatrix} $$，再没有第二种特征向量了，所以 $ A $ 不能对角化。

⚠️ 可逆和对角化之间没有必然的关系
- 可逆性（Invertibility）$ \Longleftrightarrow $ 特征值（eigenvalues）（$ \lambda's = 0 $ ❓）
- 对角化（Diagonalizability）$ \Longleftrightarrow $ 特征向量（eigenvectors）（$ \boldsymbol{x}'s $ 足够多 ❓）

对第一条的证明，互相独立的特征向量们 $ \boldsymbol{x}'s $ 对应不同的 $ \lambda's $，或者反之，一个有 $ n $ 个不同的特征值 $ \lambda's $ 的 $ n \times n $ 矩阵 一定可以对角化。

【Hint】：$ \boldsymbol{x}'s $ 的线性组合只有在系数全为零的情况下等于零（$ \boldsymbol{c}^T \boldsymbol{x} = \boldsymbol{0} $ 的系数 $ \boldsymbol{c} $ 只有零解）。

【证】假设有线性组合 $ c_1 \boldsymbol{x}_1 + c_2 \boldsymbol{x}_2 = \boldsymbol{0} $，

左乘 $ A $，得到 $ c_1 \lambda_1 \boldsymbol{x}_1 + c_2 \lambda_2 \boldsymbol{x}_2 = \boldsymbol{0} $ <br>
左乘 $ \lambda_2 $，得到 $ c_1 \lambda_2 \boldsymbol{x}_1  + c_2 \lambda_2 \boldsymbol{x}_2 = \boldsymbol{0} $

两式相减，$ c_1(\lambda_1 - \lambda_2)\boldsymbol{x}_1 = \boldsymbol{0} $。

因为所有的 $ \lambda's $ 都不一样且 $ \boldsymbol{x}_1 \neq \boldsymbol{0} $，所以等式成立只能让 $ c_1=0 $。类似可以证 $ c_2=0 $。

所以只有组合 $ c_1=c_2=0 $ 才能使 $ c_1 \boldsymbol{x}_2 + c_2 \boldsymbol{x}_2 = \boldsymbol{0} $，说明 $ \boldsymbol{x}_1,\ \boldsymbol{x}_2 $ 一定线性无关。

---

证明的思路很容易扩展到 $ j $ 个特征向量，假设有 $ c_1 \boldsymbol{x}_1 + \cdots + c_j \boldsymbol{x}_j = \boldsymbol{0} $，记为 $ S\_{j} $。

左乘 $ A $，得到 $ c_1 \lambda_1 \boldsymbol{x}_1 + \cdots + c_j \lambda_j \boldsymbol{x}_j = \boldsymbol{0} $

左乘 $ \lambda_j $，得到 $ c_1 \lambda_j \boldsymbol{x}_1 + \cdots + c_j \lambda_j \boldsymbol{x}_j = \boldsymbol{0} $

两式相减，$ c_1(\lambda_1 - \lambda_j)\boldsymbol{x}_1 + \cdots + c\_{j-1} (\lambda\_{j-1} - \lambda\_{j}) \boldsymbol{x}\_{j-1} = \boldsymbol{0} $，最后一项消掉了，现在只有 $ j-1 $ 项，记为 $ S\_{j-1} $。

对 $ S\_{j-1} $ 进行 ① 左乘 $ A $ ② 左乘 $ \lambda_{j-1} $ ③ 两式相减

可以预见，每次都会消掉一项，$ n-1 $ 次重复操作后，我们得到了 $ S_1 = c_1(\lambda_1 - \lambda_j)(\lambda_1 - \lambda_{j-1})\cdots(\lambda_1 - lambda_2) \boldsymbol{x}_1 = \boldsymbol{0} $。

同样为了在 $ \lambda's $ 互不相同的情况下使等式成立，$ c_1 $ 必须为零，同理可证 $ c_2 = \ cdots =  c_j = 0 $。

👉 **当所有特征值 $ \lambda's $ 都不一样时，有线性无关的特征向量。**

以上一节提到的 Markov 矩阵 $$ A =  \begin{bmatrix}  .8 & .3 \\ .2 & .7 \\ \end{bmatrix} $$ 为例，$ \mathrm{det}\ (A - \lambda I) = 0 $ 解出 $ \lambda_1=1,\ \lambda_2=.5 $，现在用特征值矩阵和特征向量矩阵将 $ A $ 对角化，使 $ A=X\Lambda X^{-1} $。

![diagonalize-markov](https://wx3.sinaimg.cn/large/9f1c5669ly1fvlg85b21lj20oh08cdko.jpg "Diagonalize Markov Matrix")

所以当所有 $ \lambda's < 1 $ 时，$ A^k \to \boldsymbol{0} $。


### 相似矩阵：特征值相同

我们说形如 $ A = BCB^{-1} $ 的矩阵是**与矩阵 $ C $ 相似的（similar）**，因为它们都与 $ C $ 有相同的特征值。

证明如下，

假设 $ C \boldsymbol{x} = \lambda \boldsymbol{x} $，那么 $ (BCB^{-1})(B \boldsymbol{x}) = BC \boldsymbol{x} = B \lambda \boldsymbol{x} = \lambda (B \boldsymbol{x}) $，即证 $ BCB^{-1} $ 与 $ C $ 有相同的特征值 $ \lambda $ 和新的特征向量 $ B \boldsymbol{x} $。

一个固定的 $ C $ 可以有一族的相似矩阵 $ BCB^{-1} $（所有满足乘法规则的$ B $）
- 当 $ C = I $，这家族实际上很小，$ BCB^{-1} = BIB^{-1} = I $，只有一个成员 $ I $。所以恒等矩阵 $ I $ 是唯一一个所有 $ \lambda's = 1 $ 的可对角化矩阵（有 $ n $ 个不同特征向量）。
- 如果继续令 $ \lambda's = 1 $，但是只有 $ 1 $ 个特征向量（不可对角化），这个家庭就会扩大很多。

    最简单的 $ C $ 是 Jordan 形式，比如 $$ C =  \begin{bmatrix}  1 & 1 \\ 0 & 1 \\ \end{bmatrix} $$。

    用 $ r,\ s $ 参数表示成 $$ A = BCB^{-1} = \begin{bmatrix} 1-rs & r^2 \\ -s^2 & 1+rs \\ \end{bmatrix} $$。

    检查它的行列式 $ \mathrm{det}\ A = \lambda_1 \cdot \lambda_2 = 1 $，迹 $ \mathrm{trace} = \lambda_1 + \lambda_2 = 2 $。
- 现在令 $ \lambda_1 = 1,\ \lambda_2 = 0 $，特征值现在没有重复，所以拥有相同 $ \Lambda $ 的整个家族（family）可以对角化了。

    比如特征值矩阵 $$ \Lambda =  \begin{bmatrix}  1 & 0 \\ 0 & 0 \\ \end{bmatrix} $$，家庭成员可以是 $$ A =  \begin{bmatrix}  1 & 1 \\ 0 & 0 \\ \end{bmatrix} $$ 或者 $$ A =  \begin{bmatrix}  .5 & .5 \\ .5 & .5 \\ \end{bmatrix} $$。

    或者任意的 $ A = \cfrac{ \boldsymbol{x} \boldsymbol{y}^T}{ \boldsymbol{x}^T \boldsymbol{y}} $，因为 $$ \cfrac{1}{x_1y_1+x_2y_2}\begin{bmatrix}  x_1 & x_2 \\ \end{bmatrix} \begin{bmatrix}  y_1 \\ y_2 \\ \end{bmatrix} = \cfrac{1}{x_1y_1+x_2y_2}\begin{bmatrix}  x_1y_1 & x_1y_2 \\ x_2y_1 & x_2y_2 \\ \end{bmatrix} $$。

    它的行列式 $ \mathrm{det}\ A = ad-bc = x_1y_1x_2y_2 - x_1y_2x_2y_1 = 0 $，迹 $ \mathrm{trace} = \cfrac{x_1y_1+x_2y_2}{x_1y_1+x_2y_2} = 1 $。

    这一族包含了所有满足 $ A^2 = A $ 的矩阵，因为 $ A^2 = BCB^{-1} BCB^{-1} =  BC^2B^{-1} $，又 $$ C = \Lambda = \begin{bmatrix}  1 & 0 \\ 0 & 0 \\ \end{bmatrix} $$，$ C^2 = C $，所以 $ A^2 = A $。

    当然包括 $ \Lambda $ 自身，只要让 $ B = I $ 即可。

    仔细一看，发现这个式子很像我们一维的投影矩阵 $ P = \cfrac{ \boldsymbol{a} \boldsymbol{a}^T}{ \boldsymbol{a}^T \boldsymbol{a}} $，只要 $ A $ 满足对称的条件。

    总的来说，特征值 $ 0,\ 1 $ 让生活变得简单。（和`Python`一样 🌝


### Fibonacci 数列

Fibonacci 数列 $ F_{k+2} = F_{k+1} + F_{k} $，以 $ 0,\ 1 $ 初始化，前面几个成员是 $ 0,\ 1,\ 1,\ 2,\ 3,\ 5,\ 8,\ 13,\ \cdots $。问题是，靠后的如 $ F_{100} $ 怎么求？

- 正面刚，一个一个算 ：）
- 线性代数能给出更好的办法，关键在于，如何构造 $ \boldsymbol{u}_{k+1} = A \boldsymbol{u}_k $ 这样的矩阵方程？

    $ \boldsymbol{u}_{k+1} = A \boldsymbol{u}_k $ 看起来只是对状态 $ k $ 求下**一个**状态 $ k+1 $，但是 Fibonacci 数列同时涉及到 $ k $ 和 $ k+1 $，别怕，Dont Panic！带好你的毛巾！

    把这两个状态都塞进向量 $ \boldsymbol{u}_k $ 就好了。

    令 $$ \boldsymbol{u}_k =  \begin{bmatrix}  F\_{k+1} \\ F_k \\ \end{bmatrix} $$，Fibonacci 数列的规则 $$
    \begin{align}
    F_{k+2} &= F_{k+1} + F_{k} \\
    F_{k+1} &= F_{k+1} \\
    \end{align} $$

    可以改写为矩阵方程：

    $$ \boldsymbol{u}_{k+1} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ \end{bmatrix} \boldsymbol{u}_{k} $$

    然后每一步就是乘以矩阵 $$ A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ \end{bmatrix} $$，100次乘法就有 $$ \boldsymbol{u}_{100} = A^{100} \boldsymbol{u}_0 =  \begin{bmatrix} F_{101} \\ F_{100} \\ \end{bmatrix} $$。

    ---

    1. 先算**特征值**，$$ \mathrm{det}\ (A_\lambda I) =  \begin{vmatrix} 1 - \lambda & 1 \\ 1 & - \lambda \\ \end{vmatrix} =  \lambda^2 - \lambda -1 = 0 $$。解出特征值 $ \lambda_1 = \cfrac{1+\sqrt5}{2} \approx 1.618,\ \lambda_2 = \cfrac{1-\sqrt5}{2} \approx -.618 $。

    2. 代入求**特征向量**，$ \boldsymbol{x}_1 = (\lambda_1,\ 1),\ \boldsymbol{x}_2 = (\lambda_2,\ 1) $。

    3. 再将初始值 $ \boldsymbol{u}_0 = (0,\ 1) $ **用特征向量表示**，$$ \begin{bmatrix} 1 \\ 0 \\ \end{bmatrix} = \cfrac{1}{\lambda_1 - \lambda_2}(\begin{bmatrix} \lambda_1 \\ 1 \\ \end{bmatrix}-\begin{bmatrix} \lambda_2 \\ 1 \\ \end{bmatrix})$$ 或者表示为 $ \boldsymbol{u}_0 = \cfrac{\boldsymbol{x}_1 - \boldsymbol{x}_2}{\lambda_1 - \lambda_2}$

        此时 $ \boldsymbol{u}_{100} = A^{100} \boldsymbol{u}_0 = \cfrac{A^{100}\boldsymbol{x}_1 - A^{100}\boldsymbol{x}_2}{\lambda_1 - \lambda_2} = \cfrac{\lambda_1^{100}\boldsymbol{x}_1 - \lambda_1^{100}\boldsymbol{x}_2}{\lambda_1 - \lambda_2} $。

        又因为我们只要 $$ \boldsymbol{u}_{100} = \begin{bmatrix} F_{101} \\ F_{100} \\ \end{bmatrix} $$ 的第二个元素 $ F_{100} $，所以代入 $ \boldsymbol{x}_1,\ \boldsymbol{x}_2 $ 中的第二个元素 $ 1 $ 就可以，得到 $$ F_{100} = \cfrac{\lambda_1^{100} - \lambda_1^{100}}{\lambda_1 - \lambda_2} $$。

        已知分母 $ \lambda_1 - \lambda_2 = \sqrt5 $，$ \lambda_2^{100} = (-.618)^{100} \to 0 $，所以 $ F_{100} \approx \cfrac{1}{\sqrt5} \cdot  \cfrac{1+\sqrt5}{2}^{100} $。

        $ 1.618 $ 就是常说的黄金分割，希腊人觉得这样长宽比的长方形很优雅（graceful）。


### $ A^k $

Fibonacci 数列是一个典型的**差分方程** $$ \boldsymbol{u}_{k+1} = A \boldsymbol{u}_k $$。每一步都乘以 $ A $，$ k $ 步骤之后，$ \boldsymbol{u}_{k} = A^k \boldsymbol{u}_0 $。

我们整理一下利用矩阵对角化巧妙求解 $ A^k $，得到 $ \boldsymbol{u}_k $ 的三个过程。

【Tips】对角化的原理复习， $ A=X\Lambda X^{-1},\ A^k \boldsymbol{u}_0 = (X\Lambda X^{-1}) \cdots (X\Lambda X^{-1}) \boldsymbol{u}_0 = X\Lambda^k X^{-1} \boldsymbol{u}_0 $。

1. 将 $ \boldsymbol{u}_0 $ 用特征向量 $ \boldsymbol{x}'s $ 表示成某种线性组合，$ \boldsymbol{u}_0 = c_1 \boldsymbol{x}_1 + \cdots + c_n \boldsymbol{x}_n $。$ \boldsymbol{u}_0 = X \boldsymbol{c},\ \boldsymbol{c} = X^{-1} \boldsymbol{u}_0 $。

2. 每一个特征向量 $ \boldsymbol{x}_i $ 乘以 $ (\lambda_i)^k $，矩阵化得 $ X \Lambda^k $。

3. 根据特征向量的性质 $ A \boldsymbol{x}_i = \lambda_i \boldsymbol{x} $，再结合上面两式，得到 $ \boldsymbol{u}_k = A^k \boldsymbol{u}_0 = c_1\lambda_1^k \boldsymbol{x}_1 + \cdots + c_n \lambda_n^k \boldsymbol{x}_n = X \Lambda^k \boldsymbol{c} = X \Lambda^k X^{-1} \boldsymbol{u}_0 $。

整个矩阵化的过程是对 $$ X = \begin{bmatrix} \boldsymbol{x}_1 & \cdots & \boldsymbol{x}_n  \\ \end{bmatrix} $$ 的列向量操作，所以乘法都在右侧，比如乘以特征值的对角矩阵 $ X \Lambda^k $ 或者乘基化（以特征向量为基向量）的初始 $ \boldsymbol{u}_0 $。

都放在右边之后就是 $$ X\Lambda^k X^{-1} \boldsymbol{u}_0 = X\Lambda^k \boldsymbol{c} = \begin{bmatrix} \boldsymbol{x}_1 & \cdots & \boldsymbol{x}_n \\ \end{bmatrix} \begin{bmatrix} (\lambda_1)^k & & \\ & \ddots &  \\ & & (\lambda_n)^k \\ \end{bmatrix} \begin{bmatrix} c_1  \\ \vdots \\ c_n \\ \end{bmatrix} $$。


### 练习

**Q1**. 从初始值 $$ \boldsymbol{u}_0 = \begin{bmatrix} 1 \\ 0 \\ \end{bmatrix} $$，尝试计算这个快速版的 Fibonacci 数列 $ F_{k+2} = F_{k+1} + 2F_k $，前面几个是 $ 0,\ 1,\ 1,\ 3,\ \cdots $。

$$ A = \begin{bmatrix} 1&2 \\ 1&0 \\ \end{bmatrix} $$，它的特征值和特征向量是 $$ \lambda_1 = 2,\ \boldsymbol{x}_1 = \begin{bmatrix} 2  \\ 1 \\ \end{bmatrix},\ \lambda_2 = -1,\ \boldsymbol{x}_2 = \begin{bmatrix} 1  \\ -1 \\ \end{bmatrix} $$。

利用矩阵计算 $ \boldsymbol{u}_k  = A^k \boldsymbol{u}_0 $ 仍然是三步，

1. $$ \boldsymbol{u}_0 = c_1 \boldsymbol{x}_1 + c_2 \boldsymbol{x}_2 $$，即表示为 $$\begin{bmatrix}  1 \\ 0 \\ \end{bmatrix} = \cfrac13\begin{bmatrix} 2 \\ 1 \\ \end{bmatrix} + \cfrac13\begin{bmatrix} 1 \\ -1 \\ \end{bmatrix} $$，其中 $ c_1 = c_2 = \cfrac13 $。
2. 特征向量乘以 $ \Lambda^k $，$ 2^k \boldsymbol{x}_1 + (-1)^k \boldsymbol{x}_2 $。
3. 两式结合，$$ \boldsymbol{u}_k = c_1 \lambda_1^k \boldsymbol{x}_1 + c_2 \lambda_2^k \boldsymbol{x}_2 = \cfrac13 2^k \begin{bmatrix} 2 \\ 1 \\ \end{bmatrix} + \cfrac13 (-1)^k \begin{bmatrix} 1 \\ -1 \\ \end{bmatrix} $$。

    新的 $ F_k $ 是 $ \boldsymbol{u}_k $ 的第二个元素，$ F_k = \cfrac{2^k - (-1)^k}{3} $。

    检验 $ F_4 = \cfrac{15}{3} = 5 $ 正确。

👉 **紧随特征向量（follow the eigenvectors）**，下一节我们会看到微分方程中用 $ e^{\lambda k} $ 代替了 $ \lambda^k $；傅立叶级数也有以特征向量的形式为基，$ e^{ikx} $。

**Q2**. 写出以 $$ \begin{bmatrix} 1  \\ 1 \\ \end{bmatrix},\ \begin{bmatrix} 1  \\ -1 \\ \end{bmatrix} $$ 为特征向量的矩阵的一般形式。

$ A = X \Lambda X^{-1} $，把 $ \boldsymbol{x}'s $ 放进 $ X $ 并单位化，再用 $ \lambda_1,\ \lambda_2 $ 表示 $ \Lambda $，

$$ X \Lambda X^{-1} = \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ \end{bmatrix} / \sqrt2 \begin{bmatrix} \lambda_1 & \\ & \lambda_2 \\ \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ \end{bmatrix} / \sqrt2 = \begin{bmatrix} \lambda_1 + \lambda_2 & \lambda_1 - \lambda_2 \\ \lambda_1 - \lambda_2 & \lambda_1 + \lambda_2 \\ \end{bmatrix} / 2 $$

或者写成 $$ \begin{bmatrix}  a & b \\ b & a \\ \end{bmatrix} $$，它们的特征向量都是 $ (1,\ 1),\ (1,\ -1) $。

**Q3**. 设 $ A = X \Lambda X^{-1} $，两边取行列式得 $ \mathrm{det}\ A = \mathrm{det}\ \Lambda = \lambda_1 \cdots \lambda_n $。这个简略的证明在 $ X $  可逆的情况下成立（$ \lvert X^{-1} \rvert \lvert X \rvert = 1$），即可对角化。

**Q4**. 已知 $ \mathrm{trace}\ XY = \mathrm{trace}\ YX $，求证 $ A = X \Lambda X^{-1} $ 中，$ \mathrm{trace}\ A = \mathrm{trace}\ \Lambda $。

令 $ Y=\Lambda X^{-1} $，所以 $ \mathrm{trace}\ X\Lambda X^{-1}= \mathrm{trace}\ X X^{-1}\Lambda = \mathrm{trace}\ \Lambda $，等于特征值（对角）矩阵对角线上的和。因为用到了 $ X^{-1} $，所以与上条相同，也是在 $ A $ 可对角化的情况下成立。
